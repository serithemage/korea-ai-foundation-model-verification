# Weight ë¶„ì„

> ì‹ ë¢°ë„: ë†’ìŒ | ì ‘ê·¼ì„±: ì¤‘ê°„ | Fine-tuning íƒì§€ë ¥: ì–‘í˜¸

## ê°œìš”

Weight ë¶„ì„ì€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì§ì ‘ ë¹„êµí•˜ì—¬ from scratch í•™ìŠµ ì—¬ë¶€ë¥¼ íŒë³„í•©ë‹ˆë‹¤. Fine-tuned ëª¨ë¸ì€ base modelê³¼ ë†’ì€ ê°€ì¤‘ì¹˜ ìœ ì‚¬ì„±ì„ ë³´ì´ëŠ” ë°˜ë©´, from scratch ëª¨ë¸ì€ ë…ë¦½ì ì¸ ê°€ì¤‘ì¹˜ ë¶„í¬ë¥¼ ê°–ìŠµë‹ˆë‹¤.

## ë¶„ì„ í•­ëª©

### 1. Layerë³„ Cosine Similarity ê³„ì‚°
- ê° layerì˜ weight tensor ê°„ ìœ ì‚¬ë„ ì¸¡ì •
- Fine-tuned ëª¨ë¸: ì´ˆê¸° ë ˆì´ì–´ 90% ì´ìƒ ìœ ì‚¬ë„

### 2. Weight Tensor í•´ì‹œ ë¹„êµ
- SHA-256 í•´ì‹œë¡œ ë™ì¼ì„± í™•ì¸
- ì™„ì „íˆ ë™ì¼í•œ layer íƒì§€

### 3. PCAë¥¼ í†µí•œ Weight ë¶„í¬ ë¶„ì„
- Weight matrixì˜ ì£¼ì„±ë¶„ ë¶„ì„
- From scratch: orthogonal ë¶„í¬
- Fine-tuned: base model ê·¼ì²˜ clustering

## í•´ì„ ê¸°ì¤€

### Cosine Similarity ê¸°ì¤€

| í‰ê·  ìœ ì‚¬ë„ | í•´ì„ |
|-------------|------|
| **>0.95** | ê±°ì˜ í™•ì‹¤íˆ fine-tuning |
| **0.8-0.95** | Fine-tuning ë˜ëŠ” continued pre-training |
| **0.5-0.8** | ë¶€ë¶„ì  weight ì¬ì‚¬ìš© ê°€ëŠ¥ì„± |
| **<0.5** | From scratch ê°€ëŠ¥ì„± ë†’ìŒ |

### Layerë³„ íŒ¨í„´

| íŒ¨í„´ | ì˜ë¯¸ |
|------|------|
| ì´ˆê¸° layer ë†’ì€ ìœ ì‚¬ë„, í›„ê¸° layer ë‚®ìŒ | ì „í˜•ì ì¸ fine-tuning |
| ì „ì²´ì ìœ¼ë¡œ ë‚®ì€ ìœ ì‚¬ë„ | From scratch ì¦ê±° |
| ì¼ë¶€ layerë§Œ ë†’ì€ ìœ ì‚¬ë„ | ë¶€ë¶„ì  weight ì´ˆê¸°í™” |

---

## ëª¨ë¸ë³„ ê²€ì¦ ê²°ê³¼

### 1. Upstage Solar-Open-100B âœ…

**ê²€ì¦ì¼**: 2026-01-04

#### Architecture ë¹„êµë¥¼ í†µí•œ Weight ë¹„êµ ê°€ëŠ¥ì„± ë¶„ì„

Weight ë¹„êµëŠ” ë™ì¼í•œ shapeì˜ tensor ê°„ì—ë§Œ ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.

| íŒŒë¼ë¯¸í„° | Solar-Open-100B | Mixtral-8x7B | DeepSeek-V2 | Qwen2-57B |
|----------|-----------------|--------------|-------------|-----------|
| **hidden_size** | 4,096 | 4,096 | 5,120 | 3,584 |
| **num_hidden_layers** | 48 | 32 | 60 | 28 |
| **num_attention_heads** | 64 | 32 | 128 | 28 |
| **n_routed_experts** | 128 | 8 | 160 | 64 |
| **vocab_size** | 196,608 | 32,000 | 102,400 | 151,936 |

#### íŒì •

| ë¹„êµ ëŒ€ìƒ | Weight ë¹„êµ ê°€ëŠ¥? | ì´ìœ  |
|-----------|------------------|------|
| Mixtral-8x7B | âŒ ë¶ˆê°€ | layers, heads, experts ëª¨ë‘ ë‹¤ë¦„ |
| DeepSeek-V2 | âŒ ë¶ˆê°€ | hidden_sizeë¶€í„° ë‹¤ë¦„ |
| Qwen2-57B | âŒ ë¶ˆê°€ | ëª¨ë“  dimension ë‹¤ë¦„ |

#### ê²°ë¡ 

**Weight ë¹„êµ ë¶ˆê°€ â†’ From scratch ì¦ê±°**

Fine-tuningëœ ëª¨ë¸ì´ë¼ë©´ base modelê³¼ ë™ì¼í•œ architectureë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
Solar-Open-100BëŠ” ì–´ë–¤ ê¸°ì¡´ ëª¨ë¸ê³¼ë„ architectureê°€ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
**ì§ì ‘ì ì¸ weight ë¹„êµ ì—†ì´ë„ from scratch í•™ìŠµì„ì„ ê°•ë ¥íˆ ì‹œì‚¬**í•©ë‹ˆë‹¤.

---

### 2. NAVER Cloud HyperCLOVAX-SEED-Think-32B âš ï¸

**ê²€ì¦ì¼**: 2026-01-05

#### ì»´í¬ë„ŒíŠ¸ë³„ Weight ë¹„êµ ê°€ëŠ¥ì„±

HyperCLOVAX-SEEDëŠ” VLMìœ¼ë¡œ, ì„¸ ê°€ì§€ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

| ì»´í¬ë„ŒíŠ¸ | ë¹„êµ ëŒ€ìƒ | Weight ë¹„êµ ê°€ëŠ¥? |
|----------|----------|------------------|
| **Vision Encoder** | Qwen2.5 ViT | âœ… ê°€ëŠ¥ (ë™ì¼ model_type ëª…ì‹œ) |
| **Text Decoder** | Llama 3.1 70B | âš ï¸ ë¶€ë¶„ ê°€ëŠ¥ (hidden_size ë‹¤ë¦„) |
| **Projector** | - | ìƒˆë¡œ í•™ìŠµëœ ë¶€ë¶„ |

#### Text Decoder Architecture ë¹„êµ

| íŒŒë¼ë¯¸í„° | HyperCLOVAX-SEED | Llama 3.1 70B | Qwen2.5-72B |
|----------|------------------|---------------|-------------|
| **hidden_size** | 5,120 | ~8,192 | 12,288 |
| **num_layers** | 72 | 80 | 80 |
| **num_heads** | 40 | 64 | 128 |
| **vocab_size** | 128,256 | 128,256 | ~152,000 |

#### íŒì •

| ì»´í¬ë„ŒíŠ¸ | ê²°ê³¼ | í•´ì„ |
|----------|------|------|
| Vision Encoder | Qwen2.5 ViT ì¬ì‚¬ìš© ëª…ì‹œ | âŒ From scratch ì•„ë‹˜ |
| Text Decoder | Architecture ë¶ˆì¼ì¹˜ | âš ï¸ ì¶”ê°€ ê²€ì¦ í•„ìš” |
| Tokenizer | vocab_size ì¼ì¹˜ (Llama 3) | âš ï¸ ì˜ë¬¸ì  |

**ê²°ë¡ : Vision EncoderëŠ” ì¬ì‚¬ìš© í™•ì¸, Text DecoderëŠ” ì¶”ê°€ ê²€ì¦ í•„ìš”**

---

### 3. SKT A.X-K1 ğŸ“‹

**ê²€ì¦ ìƒíƒœ**: ëŒ€ê¸° ì¤‘

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ ìœ í˜•** | MoE |
| **ì´ íŒŒë¼ë¯¸í„°** | 519B |
| **Weight ë¹„êµ** | ë¯¸ìˆ˜í–‰ |

---

### 4. NC AI VAETKI ğŸ“‹

**ê²€ì¦ ìƒíƒœ**: ëŒ€ê¸° ì¤‘

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ ìœ í˜•** | MoE |
| **ì´ íŒŒë¼ë¯¸í„°** | 112B |
| **Weight ë¹„êµ** | ë¯¸ìˆ˜í–‰ |

---

### 5. LG AI ì—°êµ¬ì› K-EXAONE ğŸ“‹

**ê²€ì¦ ìƒíƒœ**: ëŒ€ê¸° ì¤‘

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ ìœ í˜•** | MoE |
| **ì´ íŒŒë¼ë¯¸í„°** | 236B |
| **Weight ë¹„êµ** | ë¯¸ìˆ˜í–‰ |

---

## ë¶„ì„ ì½”ë“œ

### 1. Cosine Similarity ê³„ì‚°

```python
import torch
from transformers import AutoModel
import torch.nn.functional as F

def load_model_weights(model_name):
    model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)
    return {name: param.data for name, param in model.named_parameters()}

base_weights = load_model_weights("base-model")
target_weights = load_model_weights("target-model")

def cosine_sim(w1, w2):
    w1_flat = w1.flatten().float()
    w2_flat = w2.flatten().float()
    return F.cosine_similarity(w1_flat.unsqueeze(0), w2_flat.unsqueeze(0)).item()

# Layerë³„ ìœ ì‚¬ë„ ê³„ì‚°
similarities = {}
for name in base_weights:
    if name in target_weights:
        if base_weights[name].shape == target_weights[name].shape:
            sim = cosine_sim(base_weights[name], target_weights[name])
            similarities[name] = sim
            print(f"{name}: {sim:.4f}")

# í‰ê·  ìœ ì‚¬ë„
avg_sim = sum(similarities.values()) / len(similarities)
print(f"\ní‰ê·  ìœ ì‚¬ë„: {avg_sim:.4f}")
```

### 2. Weight Tensor í•´ì‹œ ë¹„êµ

```python
import hashlib
import numpy as np

def weight_hash(tensor):
    """Weight tensorì˜ SHA-256 í•´ì‹œ ê³„ì‚°"""
    arr = tensor.cpu().numpy().tobytes()
    return hashlib.sha256(arr).hexdigest()[:16]

# ë™ì¼í•œ weight íƒì§€
identical_layers = []
for name in base_weights:
    if name in target_weights:
        if base_weights[name].shape == target_weights[name].shape:
            base_hash = weight_hash(base_weights[name])
            target_hash = weight_hash(target_weights[name])
            if base_hash == target_hash:
                identical_layers.append(name)
                print(f"ë™ì¼: {name}")

print(f"\në™ì¼í•œ layer ìˆ˜: {len(identical_layers)}")
```

---

## ì£¼ì˜ì‚¬í•­

1. **ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­**: 100B ëª¨ë¸ ë¹„êµëŠ” ìƒë‹¹í•œ GPU/CPU ë©”ëª¨ë¦¬ í•„ìš”
2. **MoE êµ¬ì¡° ê³ ë ¤**: Expert weightëŠ” ë³„ë„ ë¶„ì„ í•„ìš”
3. **Quantization ì˜í–¥**: ì–‘ìí™”ëœ ëª¨ë¸ì€ í•´ì‹œ ë¹„êµ ë¶ˆê°€

---

## ê²°ë¡  ë„ì¶œ ê¸°ì¤€

**From scratch ì§€ì§€ ì¦ê±°:**
- ëª¨ë“  base modelê³¼ í‰ê·  cosine similarity 0.5 ë¯¸ë§Œ
- ë™ì¼í•œ layer ì—†ìŒ (í•´ì‹œ ë¶ˆì¼ì¹˜)
- PCAì—ì„œ ë…ë¦½ì ì¸ ë¶„í¬

**Fine-tuning ì˜ì‹¬ ì¦ê±°:**
- íŠ¹ì • base modelê³¼ 0.9 ì´ìƒ ìœ ì‚¬ë„
- ë‹¤ìˆ˜ì˜ ë™ì¼ layer (í•´ì‹œ ì¼ì¹˜)
- ì´ˆê¸° layer ë†’ì€ ìœ ì‚¬ë„ íŒ¨í„´
