# 사례 연구

실제 모델들의 검증 사례와 논란을 다룹니다.

---

## Q8: Solar-Open-100B의 행동 분석 결과는? (표절 논란과 공개 검증)

**질문 시각**: 2026-01-04

**답변**:

Solar-Open-100B에 대한 행동 분석을 진행하던 중, 2026년 1월 초 한국에서 발생한 표절 논란과 공개 검증 세션에 대한 정보를 발견했습니다. 이는 from scratch 검증에 매우 중요한 정보입니다.

2026년 1월 1일, Sionic AI CEO 고석현이 LinkedIn과 GitHub에 기술 분석을 게시하며 논란이 시작되었습니다. 그의 주장은 Solar-Open-100B와 Zhipu AI의 GLM-4.5-Air 간에 LayerNorm weight의 cosine similarity가 96.8%라는 것이었습니다. 또한 GLM 스타일 config 코드와 Zhipu AI 라이선스 참조가 발견되었다고 주장했습니다. 그의 결론은 Solar가 fine-tuned 모델이며 국가 AI 프로젝트 규정을 위반했을 가능성이 있다는 것이었습니다.

Zhipu AI의 GLM-4.5-Air와 Solar-Open-100B를 비교해보면 놀라운 유사점이 있습니다. 총 파라미터는 GLM이 106B, Solar가 102.6B입니다. 활성 파라미터는 둘 다 12B입니다. Architecture는 둘 다 MoE이고, Context Length도 둘 다 128K입니다. 다만 GLM-4.5-Air의 상세 config는 비공개입니다.

Upstage는 2026년 1월 2일 서울 강남 사무실에서 공개 검증 세션을 개최하여 대응했습니다. 이 자리에서 training checkpoints, WandB 실험 로그, 중간 산출물, 전체 학습 히스토리를 공개했습니다. Upstage의 주장은 random initialization에서 시작하여 처음부터 학습했으며, 중국 모델 가중치를 재사용하지 않았다는 것이었습니다. 코드 내 중국어 저작권 표시는 실수라고 설명했습니다.

이틀 뒤인 1월 3일, 고석현 CEO는 부분 사과를 발표하며 성급한 판단이었음을 인정했습니다.

그러나 LayerNorm 96.8% 유사도 의혹은 여전히 설명이 필요했습니다. 이에 대해 hyunwoongko가 GitHub에서 독립 검증을 수행했습니다. 그 결과 LayerNorm 96.8% 유사도 주장이 방법론적 오류였음이 밝혀졌습니다. 같은 모델의 다른 레이어 간에도 0.99 수준의 cosine similarity가 나타났습니다. 이는 LayerNorm weight가 1.0으로 초기화되어 방향적 일관성을 유지하기 때문입니다. 평균 오프셋을 제거한 centered cosine 분석을 하면 모델 간 유사도가 거의 0으로 하락했습니다. Solar가 GLM보다 Phi-3.5-MoE에 더 가깝다는 증거도 없었습니다. 결론적으로 LayerNorm 비교는 모델 기원 판별에 부적합하며, 원래 주장은 초기화 편향에 의한 false positive였습니다.

행동 분석의 한계도 있습니다. Knowledge cutoff가 공식적으로 공개되지 않아 직접 비교가 어렵습니다. Safety alignment 정보도 미공개여서 refusal pattern 분석이 제한적입니다. 직접 실행 환경이 없어 출력 스타일 비교도 제한적입니다. GLM-4.5-Air config가 미공개여서 LayerNorm 유사도를 직접 확인할 수도 없습니다.

종합하면, Upstage가 training logs와 checkpoints 등 증거를 공개했고, 외부 전문가를 초청한 공개 검증을 진행했으며, 고석현 CEO가 부분 사과했습니다. 독립 검증에서 LayerNorm 유사도 주장이 방법론적 오류로 밝혀졌습니다. 행동 분석과 독립 검증을 종합하면, from scratch 주장은 신뢰할 수 있습니다.

---

## Q9: HyperCLOVAX-SEED-Think-32B는 from scratch인가요?

**질문 시각**: 2026-01-05

**답변**:

NAVER Cloud의 HyperCLOVAX-SEED-Think-32B를 분석한 결과, 완전한 from scratch라기보다는 부분적 from scratch와 컴포넌트 재사용이 혼합된 구조로 확인되었습니다.

이 모델은 VLM(Vision-Language Model)으로 세 가지 컴포넌트로 구성됩니다. Vision Encoder, Text Decoder, 그리고 Projector입니다.

가장 명확한 발견은 Vision Encoder가 Qwen2.5 ViT를 재사용한다는 것입니다. config.json에 `"model_type": "qwen2_5_vl"`이 명시되어 있습니다. 따라서 Vision 부분은 from scratch가 아닙니다.

Tokenizer에 대해서는 흥미로운 발견이 있었습니다. HyperCLOVAX-SEED의 vocab_size는 128,256개입니다. Llama 3/3.1은 128,000개로 256 토큰 차이가 납니다. 처음에는 Llama 3 tokenizer를 재사용했다고 생각할 수 있지만, 정확히 일치하지 않습니다. Trillion-7B도 128,256개로 동일한 vocab 설계를 사용합니다. Trillion-7B 논문에 따르면 이 128,256 vocab은 약 100k 영어와 약 24.5k 한국어로 구성된 한국어 최적화 설계입니다. 단순히 Llama 3 tokenizer를 재사용한 것이 아닐 가능성이 있습니다. 다만 HyperCLOVA X 논문에서 언급된 100k vocab에서 SEED 버전의 128k vocab으로 변경된 이유가 공식 문서화되지 않아 완전한 결론은 어렵습니다.

Text Decoder의 architecture를 보면 고유한 요소가 있습니다. model_type이 "hyperclovax"로 독자적입니다. hidden_size는 5,120으로 Llama 3.1 70B의 약 8,192나 Qwen2.5-72B의 12,288과 다릅니다. num_layers는 72개로 Llama와 Qwen의 80개와 다릅니다. num_heads는 40개로 역시 다릅니다. 특히 주목할 점은 rope_theta가 50,000,000이라는 것입니다. Llama 3.1은 500,000, Qwen2.5는 1,000,000인데, HyperCLOVAX는 50M으로 다른 모델에서 볼 수 없는 고유값입니다.

결론적으로 완전한 from scratch라고 보기 어렵습니다. Vision Encoder가 Qwen2.5 ViT를 그대로 사용한다는 점이 config.json에 명시되어 있어서, VLM에서 Vision 부분은 확실히 재사용했습니다. Tokenizer에 대해서는 Llama 3와 256 토큰 차이가 있어 단순 재사용이 아닐 수 있지만, 추가 검증이 필요합니다. Text Decoder는 architecture가 고유하지만 이것만으로 from scratch를 확정하기 어렵습니다.

---

<!-- SECTION_MARKER: 새로운 사례연구 Q&A는 이 마커 위에 추가됩니다 -->
