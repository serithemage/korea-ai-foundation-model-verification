# Architecture ë¶„ì„

> ì‹ ë¢°ë„: ì¤‘ê°„ | ì ‘ê·¼ì„±: ë†’ìŒ | Fine-tuning íƒì§€ë ¥: ì–‘í˜¸

## ê°œìš”

Architecture ë¶„ì„ì€ ëª¨ë¸ì˜ êµ¬ì¡°ì  íŠ¹ì§•ì„ ë¹„êµí•˜ì—¬ ê¸°ì›ì„ ì¶”ì í•©ë‹ˆë‹¤. ë™ì¼í•œ architecture configëŠ” fine-tuningì˜ ê°•ë ¥í•œ ì¦ê±°ê°€ ë˜ë©°, ê³ ìœ í•œ êµ¬ì¡°ì  íŠ¹ì§•ì€ from scratch í•™ìŠµì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

## ë¶„ì„ í•­ëª©

### 1. ê¸°ë³¸ Hyperparameters ë¹„êµ
- ë ˆì´ì–´ ìˆ˜, hidden dimension, attention heads
- Intermediate size, vocabulary size

### 2. í™œì„±í™” í•¨ìˆ˜ ë° Normalization
- SiLU, GELU, ReLU ë“±
- RMSNorm vs LayerNorm

### 3. ê³ ìœ í•œ êµ¬ì¡°ì  íŠ¹ì§•
- RoPE scaling ë°©ì‹
- Attention êµ¬í˜„ (GQA, MQA, MHA)
- MoE êµ¬ì„± (expert ìˆ˜, top-k)

## ê³ ìœ ì„± íŒë‹¨ ê¸°ì¤€

### ë™ì¼ config íŒì •

ë‹¤ìŒ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•˜ë©´ ë™ì¼ architectureë¡œ íŒì •:

1. `hidden_size` ì¼ì¹˜
2. `num_hidden_layers` ì¼ì¹˜
3. `num_attention_heads` ì¼ì¹˜
4. `intermediate_size` ì¼ì¹˜
5. `hidden_act` ì¼ì¹˜

### íŒŒìƒ ëª¨ë¸ ê°€ëŠ¥ì„± ì§€í‘œ

| ì¼ì¹˜ í•­ëª© | í•´ì„ |
|----------|------|
| 5/5 | ë™ì¼ architecture - fine-tuning ì˜ì‹¬ |
| 3-4/5 | ìœ ì‚¬ architecture - ì°¸ì¡° ê°€ëŠ¥ì„± |
| 1-2/5 | ë…ë¦½ì  ì„¤ê³„ ê°€ëŠ¥ì„± |
| 0/5 | ì™„ì „íˆ ë‹¤ë¥¸ architecture |

---

## ëª¨ë¸ë³„ ê²€ì¦ ê²°ê³¼

### 1. Upstage Solar-Open-100B âœ…

**ê²€ì¦ì¼**: 2026-01-04

#### ê¸°ë³¸ ì •ë³´

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ ìœ í˜•** | Mixture-of-Experts (MoE) |
| **ì´ íŒŒë¼ë¯¸í„°** | 102.6B |
| **í™œì„± íŒŒë¼ë¯¸í„°** | 12B (í† í°ë‹¹) |
| **Expert êµ¬ì„±** | 129ê°œ (128 routed + 1 shared, top-8 í™œì„±í™”) |
| **Context Length** | 128k tokens |

#### Architecture ë¹„êµ ìš”ì•½

| íŒŒë¼ë¯¸í„° | Solar-Open-100B | Mixtral | DeepSeek-V2 | Qwen2-57B | ì¼ì¹˜ ëª¨ë¸ |
|----------|-----------------|---------|-------------|-----------|----------|
| hidden_size | 4,096 | 4,096 | 5,120 | 3,584 | Mixtralë§Œ |
| num_layers | 48 | 32 | 60 | 28 | ì—†ìŒ |
| num_heads | 64 | 32 | 128 | 28 | ì—†ìŒ |
| num_kv_heads | 8 | 8 | 128 | 4 | Mixtralë§Œ |
| n_experts | 128+1 | 8 | 160+2 | 64 | ì—†ìŒ |
| vocab_size | 196,608 | 32,000 | 102,400 | 151,936 | ì—†ìŒ |
| rope_theta | 1,000,000 | 1,000,000 | 10,000 | 1,000,000 | Mixtral, Qwen |

#### íŒì •

| ì¼ì¹˜ í•­ëª© ìˆ˜ | ë¹„êµ ëŒ€ìƒ | ê²°ê³¼ |
|-------------|----------|------|
| **2/7** | Mixtral | hidden_size, kv_headsë§Œ ì¼ì¹˜ |
| **1/7** | DeepSeek-V2 | rope_theta ê³„ì—´ë§Œ ìœ ì‚¬ |
| **1/7** | Qwen2-57B | rope_thetaë§Œ ë™ì¼ |

#### ê³ ìœ  íŠ¹ì§•

1. **129ê°œ Expert êµ¬ì„±** (128 routed + 1 shared) - ë‹¤ë¥¸ ëª¨ë¸ì—ì„œ ë³¼ ìˆ˜ ì—†ëŠ” êµ¬ì„±
2. **48 layers** - Mixtral(32)ê³¼ DeepSeek(60)ì˜ ë‹¨ìˆœ ì¤‘ê°„ê°’ ì•„ë‹˜
3. **64 attention heads** - ê°€ì¥ ë§ì€ head ìˆ˜ (Dense ëª¨ë¸ ì œì™¸)
4. **moe_intermediate_size: 1,280** - ë¹„êµ ëŒ€ìƒ ì¤‘ ê°€ì¥ ì‘ìŒ (íš¨ìœ¨ì  ì„¤ê³„)
5. **vocab_size: 196,608** - ëª¨ë“  ë¹„êµ ëŒ€ìƒ ì¤‘ ê°€ì¥ í¼

**ê²°ë¡ : 0/5 ì™„ì „ ì¼ì¹˜ â†’ ë…ë¦½ì  ì„¤ê³„ (From scratch ì§€ì§€)**

---

### 2. NAVER Cloud HyperCLOVAX-SEED-Think-32B âš ï¸

**ê²€ì¦ì¼**: 2026-01-05

#### ê¸°ë³¸ ì •ë³´

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ ìœ í˜•** | Dense (Vision-Language Model) |
| **ì´ íŒŒë¼ë¯¸í„°** | 32B (33B params) |
| **Context Length** | 128K tokens |
| **Knowledge Cutoff** | 2025ë…„ 5ì›” |

#### ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°

HyperCLOVAX-SEED-Think-32BëŠ” **VLM**ìœ¼ë¡œ ì„¸ ê°€ì§€ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HCXVisionV2ForCausalLM                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Vision Encoder â”‚â†’â”‚ Projectorâ”‚â†’â”‚  Text Decoder   â”‚ â”‚
â”‚  â”‚  (Qwen2.5 ViT)  â”‚  â”‚ (Linear) â”‚  â”‚ (HyperCLOVAX)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Text Decoder Config ë¹„êµ

| íŒŒë¼ë¯¸í„° | HyperCLOVAX-SEED-32B | Llama 3.1 70B | Qwen2.5-72B |
|----------|---------------------|---------------|-------------|
| **model_type** | hyperclovax | llama | qwen2 |
| **hidden_size** | 5,120 | ~8,192 | 12,288 |
| **num_hidden_layers** | 72 | 80 | 80 |
| **num_attention_heads** | 40 | 64 | 128 |
| **num_key_value_heads** | 8 | 8 | 8 |
| **vocab_size** | 128,256 | 128,256 | ~152,000 |
| **rope_theta** | 50,000,000 | 500,000 | 1,000,000 |

#### Vision Encoder Config

| íŒŒë¼ë¯¸í„° | ê°’ | ë¹„ê³  |
|----------|-----|------|
| **model_type** | qwen2_5_vl | **Qwen2.5 Vision Transformer ì‚¬ìš©** |
| **hidden_size** | 1,280 | |
| **out_hidden_size** | 5,120 | Text decoder hidden_sizeì™€ ì¼ì¹˜ |
| **depth** | 32 | |
| **num_heads** | 16 | |

#### ê³ ìœ  ìš”ì†Œ

1. `model_type: hyperclovax` - ê³ ìœ í•œ ëª¨ë¸ íƒ€ì…
2. `rope_theta: 50,000,000` - Llama 3 (500k), Qwen2.5 (1M)ë³´ë‹¤ í›¨ì”¬ í¼
3. `attention_multiplier: 0.08838834764831845` - ê³ ìœ í•œ ì„¤ì •
4. 72 layers, 40 heads - ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ì¡°í•©

#### íŒì •

| ì»´í¬ë„ŒíŠ¸ | ê²°ê³¼ | From scratch ì§€ì§€ |
|----------|------|------------------|
| **Text Decoder** | ê³ ìœ  architecture | âœ… ì§€ì§€ |
| **Vision Encoder** | Qwen2.5 ViT ì‚¬ìš© | âŒ ì¬ì‚¬ìš© |
| **rope_theta** | 50M (ê³ ìœ ê°’) | âœ… ì§€ì§€ |
| **vocab_size** | Llama 3ì™€ ë™ì¼ | âš ï¸ ì˜ë¬¸ì  |

**ê²°ë¡ : ë¶€ë¶„ì  ì¬ì‚¬ìš© (Vision EncoderëŠ” from scratch ì•„ë‹˜)**

---

### 3. SKT A.X-K1 âœ…

**ê²€ì¦ì¼**: 2026-01-05

#### ê¸°ë³¸ ì •ë³´

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ ìœ í˜•** | Mixture-of-Experts (MoE) |
| **model_type** | AXK1 (ê³ ìœ ) |
| **ì´ íŒŒë¼ë¯¸í„°** | 519B |
| **í™œì„± íŒŒë¼ë¯¸í„°** | ~22B (í† í°ë‹¹, top-8 experts) |
| **Expert êµ¬ì„±** | 193ê°œ (192 routed + 1 shared, top-8 í™œì„±í™”) |
| **Context Length** | 131,072 tokens (YaRN RoPE scaling) |

#### Architecture ë¹„êµ ìš”ì•½

| íŒŒë¼ë¯¸í„° | A.X-K1 | Solar-Open-100B | DeepSeek-V2 | Qwen2-57B | ì¼ì¹˜ ëª¨ë¸ |
|----------|--------|-----------------|-------------|-----------|----------|
| hidden_size | 7,168 | 4,096 | 5,120 | 3,584 | ì—†ìŒ |
| num_layers | 61 | 48 | 60 | 28 | ì—†ìŒ |
| num_heads | 64 | 64 | 128 | 28 | Solarë§Œ |
| num_kv_heads | 64 (MHA) | 8 (GQA) | 128 | 4 | ì—†ìŒ |
| n_experts | 192+1 | 128+1 | 160+2 | 64 | ì—†ìŒ |
| experts_per_tok | 8 | 8 | 6 | 8 | Solar, Qwen |
| vocab_size | 163,840 | 196,608 | 102,400 | 151,936 | ì—†ìŒ |
| rope_theta | 10,000 | 1,000,000 | 10,000 | 1,000,000 | DeepSeekë§Œ |
| intermediate_size | 18,432 | N/A | 12,288 | 2,560 | ì—†ìŒ |

#### Attention êµ¬ì¡°

| í•­ëª© | ê°’ | ë¹„ê³  |
|------|-----|------|
| **Attention Type** | MHA (Multi-Head Attention) | num_heads = num_kv_heads = 64 |
| **Head Dimension** | 112 (7168 / 64) | |
| **Q Lora Rank** | 1,536 | Low-rank attention ì‚¬ìš© |
| **KV Lora Rank** | 512 | |

A.X-K1ì€ GQAê°€ ì•„ë‹Œ **MHA(Multi-Head Attention)**ì„ ì‚¬ìš©í•˜ë©°, Low-rank projectionì„ ì ìš©í•©ë‹ˆë‹¤.

#### MoE êµ¬ì¡°

| í•­ëª© | ê°’ | ë¹„ê³  |
|------|-----|------|
| **Routed Experts** | 192 | ê°€ì¥ ë§ì€ expert ìˆ˜ |
| **Shared Experts** | 1 | ëª¨ë“  í† í°ì— í™œì„±í™” |
| **Top-k** | 8 | Solarì™€ ë™ì¼ |
| **MoE Intermediate Size** | 2,560 | |
| **Scoring Function** | softmax | |
| **Norm Top-k Prob** | True | |

#### RoPE Scaling (YaRN)

| í•­ëª© | ê°’ |
|------|-----|
| **type** | yarn |
| **factor** | 4.0 |
| **original_max_position_embeddings** | 32,768 |
| **beta_fast** | 32.0 |
| **beta_slow** | 1.0 |
| **mscale** | 1.0 |
| **mscale_all_dim** | 0.0 |

YaRN scalingì„ í†µí•´ 32K â†’ 131K context length í™•ì¥.

#### ê³ ìœ  íŠ¹ì§•

1. **model_type: AXK1** - ì™„ì „íˆ ê³ ìœ í•œ ëª¨ë¸ íƒ€ì…
2. **hidden_size: 7,168** - ëª¨ë“  ë¹„êµ ëŒ€ìƒ ì¤‘ ê°€ì¥ í¼
3. **193ê°œ Expert êµ¬ì„±** (192 routed + 1 shared) - ê°€ì¥ ë§ì€ expert ìˆ˜
4. **MHA ì‚¬ìš©** - ìµœì‹  MoE ëª¨ë¸ë“¤ì´ GQAë¥¼ ì„ í˜¸í•˜ëŠ” ì¶”ì„¸ì™€ ë‹¤ë¦„
5. **vocab_size: 163,840** - ëª¨ë“  ë¹„êµ ëŒ€ìƒê³¼ ë¶ˆì¼ì¹˜
6. **Low-rank Attention** - Q/KVì— LoRA rank ì ìš©

#### íŒì •

| ì¼ì¹˜ í•­ëª© ìˆ˜ | ë¹„êµ ëŒ€ìƒ | ê²°ê³¼ |
|-------------|----------|------|
| **1/9** | Solar-Open-100B | num_headsë§Œ ì¼ì¹˜ |
| **1/9** | DeepSeek-V2 | rope_thetaë§Œ ì¼ì¹˜ |
| **1/9** | Qwen2-57B | experts_per_tokë§Œ ì¼ì¹˜ |

**ê²°ë¡ : 0/5 í•µì‹¬ í•­ëª© ì™„ì „ ì¼ì¹˜ â†’ ë…ë¦½ì  ì„¤ê³„ (From scratch ì§€ì§€)**

---

### 4. NC AI VAETKI ğŸ“‹

**ê²€ì¦ ìƒíƒœ**: ëŒ€ê¸° ì¤‘

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ ìœ í˜•** | MoE |
| **ì´ íŒŒë¼ë¯¸í„°** | 112B |
| **Architecture ë¶„ì„** | ë¯¸ìˆ˜í–‰ |

---

### 5. LG AI ì—°êµ¬ì› K-EXAONE ğŸ“‹

**ê²€ì¦ ìƒíƒœ**: ëŒ€ê¸° ì¤‘

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ ìœ í˜•** | MoE |
| **ì´ íŒŒë¼ë¯¸í„°** | 236B |
| **Architecture ë¶„ì„** | ë¯¸ìˆ˜í–‰ |

---

## ì°¸ì¡°ìš© ëª¨ë¸ ë¹„êµí‘œ

| ëª¨ë¸ | Type | Layers | Hidden | Heads | KV Heads | Experts | Vocab |
|------|------|--------|--------|-------|----------|---------|-------|
| **Solar-Open-100B** | MoE | 48 | 4,096 | 64 | 8 | 128+1 | 196,608 |
| **HyperCLOVAX-SEED** | Dense | 72 | 5,120 | 40 | 8 | - | 128,256 |
| **A.X-K1** | MoE | 61 | 7,168 | 64 | 64 | 192+1 | 163,840 |
| Mixtral-8x7B | MoE | 32 | 4,096 | 32 | 8 | 8 | 32,000 |
| DeepSeek-V2 | MoE | 60 | 5,120 | 128 | 128 | 160+2 | 102,400 |
| Qwen2-57B-A14B | MoE | 28 | 3,584 | 28 | 4 | 64 | 151,936 |
| Llama-3-70B | Dense | 80 | 8,192 | 64 | 8 | - | 128,256 |

---

## ë¶„ì„ ì½”ë“œ

### config.json ë¹„êµ

```python
from transformers import AutoConfig

def compare_configs(model_names):
    configs = {}
    for name in model_names:
        configs[name] = AutoConfig.from_pretrained(name)

    # ì£¼ìš” í•­ëª© ë¹„êµ
    keys = [
        "hidden_size",
        "intermediate_size",
        "num_hidden_layers",
        "num_attention_heads",
        "num_key_value_heads",
        "vocab_size",
        "max_position_embeddings",
        "rms_norm_eps",
        "rope_theta",
        "hidden_act",
    ]

    for key in keys:
        values = []
        for name in model_names:
            val = getattr(configs[name], key, "N/A")
            values.append(str(val))
        print(f"{key}: {values}")
```

### Attention êµ¬ì¡° ë¶„ì„

```python
def analyze_attention(config):
    num_heads = config.num_attention_heads
    num_kv_heads = getattr(config, "num_key_value_heads", num_heads)

    if num_kv_heads == num_heads:
        attn_type = "MHA (Multi-Head Attention)"
    elif num_kv_heads == 1:
        attn_type = "MQA (Multi-Query Attention)"
    else:
        attn_type = f"GQA (Grouped Query Attention, {num_heads//num_kv_heads} groups)"

    return attn_type
```

---

## ê²°ë¡  ë„ì¶œ ê¸°ì¤€

**From scratch ì§€ì§€ ì¦ê±°:**
- ëª¨ë“  ì£¼ìš” ëª¨ë¸ê³¼ architecture ë¶ˆì¼ì¹˜
- ê³ ìœ í•œ MoE êµ¬ì„± (129 experts)
- ë¹„í‘œì¤€ hyperparameter ì¡°í•©

**Fine-tuning ì˜ì‹¬ ì¦ê±°:**
- íŠ¹ì • ëª¨ë¸ê³¼ ì™„ì „í•œ config ì¼ì¹˜
- ë™ì¼í•œ hidden_size, layers, heads ì¡°í•©
- í‘œì¤€ì ì¸ RoPE ì„¤ì •
