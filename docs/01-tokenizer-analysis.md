# Tokenizer ë¶„ì„

> ì‹ ë¢°ë„: ë†’ìŒ | ì ‘ê·¼ì„±: ë†’ìŒ | Fine-tuning íƒì§€ë ¥: ìš°ìˆ˜

## ê°œìš”

Tokenizer ë¶„ì„ì€ LLMì´ from scratchë¡œ í•™ìŠµë˜ì—ˆëŠ”ì§€ íŒë³„í•˜ëŠ” ê°€ì¥ ì ‘ê·¼ì„± ë†’ì€ ë°©ë²•ì…ë‹ˆë‹¤. Fine-tuning ì‹œ tokenizerë¥¼ ì¬í•™ìŠµí•˜ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ê¸° ë•Œë¬¸ì—, tokenizerì˜ ìœ ì‚¬ì„±ì€ ëª¨ë¸ ê¸°ì›ì„ ì¶”ì í•˜ëŠ” ê°•ë ¥í•œ ì§€í‘œê°€ ë©ë‹ˆë‹¤.

## ë¶„ì„ í•­ëª©

### 1. Vocabulary ë¹„êµ
- ê¸°ì¡´ base modelë“¤ê³¼ì˜ í† í° ì¤‘ë³µë¥  í™•ì¸
- ê³ ìœ  í† í° ì‹ë³„

### 2. BPE Merge Rules ë¶„ì„
- Merge ìˆœì„œ ë° íŒ¨í„´ ë¹„êµ
- ë™ì¼í•œ merge rulesëŠ” ê°™ì€ tokenizer ì¦ê±°

### 3. íŠ¹ìˆ˜ í† í° íŒ¨í„´ ë¹„êµ
- `<eos>`, `<pad>`, `<bos>`, `<unk>` ë“±
- Chat template í† í° (`<|im_start|>`, `[INST]` ë“±)

## Tokenizer ì‘ë™ ì›ë¦¬

| ë°©ì‹ | íŠ¹ì§• | ì‚¬ìš© ëª¨ë¸ |
|------|------|----------|
| **BPE** (Byte Pair Encoding) | ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ì¸ì ‘ ë¬¸ììŒ ë³‘í•© | GPT-2, RoBERTa |
| **WordPiece** | likelihood ìµœëŒ€í™” ê¸°ì¤€ ë³‘í•© | BERT |
| **SentencePiece** | ê³µë°± í¬í•¨ ì›ì‹œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (â– ë§ˆì»¤ ì‚¬ìš©) | T5, Gemma, Llama |

## Fine-tuning ì‹œ Tokenizerë¥¼ ì¬í•™ìŠµí•˜ì§€ ì•ŠëŠ” ì´ìœ 

1. **Embedding í˜¸í™˜ì„±**: ìƒˆ vocabularyëŠ” pre-trained embeddingê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŒ
2. **ë¹„ìš©**: Tokenizer ì¬í•™ìŠµì€ ì „ì²´ corpus ì¬ì²˜ë¦¬ í•„ìš”
3. **ì„±ëŠ¥ ì €í•˜ ìœ„í—˜**: Vocabulary ë³€ê²½ ì‹œ ì›ë˜ ë„ë©”ì¸ ì„±ëŠ¥ ì €í•˜

## Vocabulary ì¤‘ë³µë¥  í•´ì„ ê¸°ì¤€

| ì¤‘ë³µë¥  | í•´ì„ |
|--------|------|
| **>98%** | Fine-tuning ê°€ëŠ¥ì„± ë†’ìŒ (safety token ë“± ì†ŒëŸ‰ ì¶”ê°€ë§Œ) |
| **90-98%** | Continued pre-training ë˜ëŠ” vocabulary í™•ì¥ |
| **<90%** | From scratch í•™ìŠµ ê°•ë ¥ ì¦ê±° |

---

## ëª¨ë¸ë³„ ê²€ì¦ ê²°ê³¼

### 1. Upstage Solar-Open-100B âœ…

**ê²€ì¦ì¼**: 2026-01-04

#### Vocabulary í¬ê¸° ë¹„êµ

| ëª¨ë¸ | Vocab Size | Tokenizer Type | Solar ëŒ€ë¹„ |
|------|-----------|----------------|------------|
| **Solar-Open-100B** | **196,608** | SentencePiece (BPE) | - |
| Qwen2-72B | 152,064 | BPE | -23% |
| Llama-3 | 128,256 | tiktoken (BPE) | -35% |
| DeepSeek-V2 | 102,400 | BPE | -48% |
| Mixtral-8x7B | 32,000 | SentencePiece | -84% |

#### Special Tokens ë¹„êµ

| ëª¨ë¸ | bos_token | eos_token | pad_token |
|------|-----------|-----------|-----------|
| **Solar-Open-100B** | `<s>` | `</s>` | `<pad>` |
| Llama-3 | `<\|begin_of_text\|>` | `<\|end_of_text\|>` | (ì—†ìŒ) |
| Mixtral | `<s>` | `</s>` | (ì—†ìŒ) |

#### íŒì •

| ì§€í‘œ | ê²°ê³¼ | í•´ì„ |
|------|------|------|
| Vocab Size ì¼ì¹˜ | 0ê°œ ëª¨ë¸ | âœ… From scratch ì§€ì§€ |
| Special Tokens | Mixtralê³¼ ìœ ì‚¬ | âš ï¸ ì¤‘ë¦½ |
| Tokenizer Type | ê³µí†µ ë°©ì‹ | âš ï¸ ì¤‘ë¦½ |

**ê²°ë¡ : From scratch í•™ìŠµ ì£¼ì¥ ì§€ì§€**

---

### 2. NAVER Cloud HyperCLOVAX-SEED-Think-32B âš ï¸

**ê²€ì¦ì¼**: 2026-01-05

#### Vocabulary í¬ê¸° ë¹„êµ

| ëª¨ë¸ | Vocab Size | ë¹„ê³  |
|------|-----------|------|
| **HyperCLOVAX-SEED** | **128,256** | Llama 3ì™€ ë™ì¼ |
| Llama 3 | 128,256 | ì •í™•íˆ ì¼ì¹˜ |
| HyperCLOVA X (ë…¼ë¬¸) | 100,000 | "SEED" ë²„ì „ê³¼ ë‹¤ë¦„ |

#### Special Tokens ë¹„êµ

| í† í° | ê°’ | ë¹„ê³  |
|------|-----|------|
| `<\|IMAGE_PAD\|>` | Visionìš© | VLM íŠ¹í™” |
| `<\|im_start\|>`, `<\|im_end\|>` | Conversation | ChatML ìŠ¤íƒ€ì¼ |
| `<\|fim_prefix\|>`, `<\|fim_middle\|>`, `<\|fim_suffix\|>` | Code | Fill-in-the-middle |

#### íŒì •

| ì§€í‘œ | ê²°ê³¼ | í•´ì„ |
|------|------|------|
| **Vocab Size** | Llama 3ì™€ ë™ì¼ (128,256) | âš ï¸ ì˜ë¬¸ì  |
| **Special Tokens** | ë…ìì  êµ¬ì„± | âœ… ì§€ì§€ |
| **ë…¼ë¬¸ ë¶ˆì¼ì¹˜** | HyperCLOVA X(100k) vs SEED(128k) | âš ï¸ ì¶”ê°€ ê²€ì¦ í•„ìš” |

**ê²°ë¡ : ì¶”ê°€ ê²€ì¦ í•„ìš” (vocab_sizeê°€ Llama 3ì™€ ì •í™•íˆ ì¼ì¹˜)**

---

### 3. SKT A.X-K1 ğŸ“‹

**ê²€ì¦ ìƒíƒœ**: ëŒ€ê¸° ì¤‘

| í•­ëª© | ê°’ |
|------|-----|
| **ì˜ˆìƒ Vocab Size** | ë¯¸í™•ì¸ |
| **Tokenizer Type** | ë¯¸í™•ì¸ |

---

### 4. NC AI VAETKI ğŸ“‹

**ê²€ì¦ ìƒíƒœ**: ëŒ€ê¸° ì¤‘

| í•­ëª© | ê°’ |
|------|-----|
| **ì˜ˆìƒ Vocab Size** | ë¯¸í™•ì¸ |
| **Tokenizer Type** | ë¯¸í™•ì¸ |

---

### 5. LG AI ì—°êµ¬ì› K-EXAONE ğŸ“‹

**ê²€ì¦ ìƒíƒœ**: ëŒ€ê¸° ì¤‘

| í•­ëª© | ê°’ |
|------|-----|
| **ì˜ˆìƒ Vocab Size** | ë¯¸í™•ì¸ |
| **Tokenizer Type** | ë¯¸í™•ì¸ |

---

## ë¶„ì„ ì½”ë“œ

### 1. Vocabulary ë¹„êµ

```python
from transformers import AutoTokenizer

base_tok = AutoTokenizer.from_pretrained("base-model")
target_tok = AutoTokenizer.from_pretrained("target-model")

base_vocab = set(base_tok.get_vocab().keys())
target_vocab = set(target_tok.get_vocab().keys())

overlap = len(base_vocab & target_vocab)
overlap_pct = (overlap / len(base_vocab)) * 100
print(f"ì¤‘ë³µë¥ : {overlap_pct:.2f}%")

# ê³ ìœ  í† í° í™•ì¸
only_in_base = base_vocab - target_vocab
only_in_target = target_vocab - base_vocab
print(f"Baseì—ë§Œ ìˆëŠ” í† í°: {len(only_in_base)}")
print(f"Targetì—ë§Œ ìˆëŠ” í† í°: {len(only_in_target)}")
```

### 2. Merge Rules ë¹„êµ (BPE/SentencePiece)

```python
# mergesê°€ ë™ì¼í•˜ë©´ ê°™ì€ tokenizer
base_merges = base_tok.backend_tokenizer.model.get_vocab()
target_merges = target_tok.backend_tokenizer.model.get_vocab()

# ì²« 100ê°œ merge ë¹„êµ
merge_match = sum(1 for i in range(min(100, len(base_merges), len(target_merges)))
                  if list(base_merges.items())[i] == list(target_merges.items())[i])
print(f"ì²« 100ê°œ merge ì¼ì¹˜ìœ¨: {merge_match}%")
```

### 3. Special Tokens ë¹„êµ

```python
print("Base special tokens:", base_tok.special_tokens_map)
print("Target special tokens:", target_tok.special_tokens_map)

# ì¶”ê°€ëœ special tokens í™•ì¸
print("Added tokens:", target_tok.added_tokens_encoder)
```

---

## ê²°ë¡  ë„ì¶œ ê¸°ì¤€

**From scratch ì§€ì§€ ì¦ê±°:**
- ëª¨ë“  ì£¼ìš” base modelê³¼ vocabulary ì¤‘ë³µë¥  90% ë¯¸ë§Œ
- ê³ ìœ í•œ special token ì²´ê³„
- ë…ìì ì¸ merge rules

**Fine-tuning ì˜ì‹¬ ì¦ê±°:**
- íŠ¹ì • base modelê³¼ 95% ì´ìƒ vocabulary ì¤‘ë³µ
- ë™ì¼í•œ special token íŒ¨í„´
- merge rules ì¼ì¹˜
